{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Anjali Parwani D11AD 45 EXP7**"
      ],
      "metadata": {
        "id": "FP70_2F_jzlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: Perform the steps involved in Text Analysis in Python and R"
      ],
      "metadata": {
        "id": "AZVQD9najzoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task to be performed :\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "Perform the following experiments using Python & R"
      ],
      "metadata": {
        "id": "mXbxu_fSjzrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Nltk Library"
      ],
      "metadata": {
        "id": "w33O03venVXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "import nltk.corpus\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8UcKv-Um3hu",
        "outputId": "03ff42d0-a7a5-4869-f917-988e07441d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMB5G56tpF29",
        "outputId": "08aefe69-13d7-4574-c725-73d4da29a6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "filtered_tokens = [word for word in token if word.lower() not in stopwords.words('english')]\n",
        "print(\"Tokens after removing stopwords:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NylMYqZm3kM",
        "outputId": "e4403831-d81e-45dc-c400-b68e95091929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after removing stopwords:\n",
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RT8aQQJstUe",
        "outputId": "d6975c64-c75e-4d43-9467-9bce6c1a49a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "tags = nltk.pos_tag(words)\n",
        "\n",
        "# Define chunk grammar using regular expressions\n",
        "chunk_grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN>}    # Chunk sequences of DT, JJ, NN\n",
        "\"\"\"\n",
        "\n",
        "# Create a chunk parser\n",
        "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
        "\n",
        "# Perform chunking\n",
        "chunked = chunk_parser.parse(tags)\n",
        "\n",
        "# Print the chunked result\n",
        "print(chunked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_W3FN_trasl",
        "outputId": "6a053f40-da68-403f-dc19-019979c51069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"easily\", \"fairly\", \"fairness\"]\n",
        "\n",
        "for word in words:\n",
        "    stemmed_word = porter_stemmer.stem(word)\n",
        "    print(f\"{word} -> {stemmed_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlUQqYw2SUNF",
        "outputId": "cb84de05-65b1-45df-84ae-c391ed3fff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "easily -> easili\n",
            "fairly -> fairli\n",
            "fairness -> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Spacy library"
      ],
      "metadata": {
        "id": "FCL6bYBPpm-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"spaCy is an open-source software library for advanced natural language processing, written in Python and Cython.\"\n",
        "\n",
        "# Tokenization\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"spaCy Tokenization:\")\n",
        "print(tokens)\n",
        "\n",
        "# Part-of-speech tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"Part-of-speech tagging:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CI8WxgBm3n8",
        "outputId": "e2198a5c-a870-4817-9677-20db2231f40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Tokenization:\n",
            "['spaCy', 'is', 'an', 'open', '-', 'source', 'software', 'library', 'for', 'advanced', 'natural', 'language', 'processing', ',', 'written', 'in', 'Python', 'and', 'Cython', '.']\n",
            "Part-of-speech tagging:\n",
            "[('spaCy', 'INTJ'), ('is', 'AUX'), ('an', 'DET'), ('open', 'ADJ'), ('-', 'PUNCT'), ('source', 'NOUN'), ('software', 'NOUN'), ('library', 'NOUN'), ('for', 'ADP'), ('advanced', 'ADJ'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), (',', 'PUNCT'), ('written', 'VERB'), ('in', 'ADP'), ('Python', 'PROPN'), ('and', 'CCONJ'), ('Cython', 'PROPN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"running easily fairly fairness\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXYwDMIYRKaS",
        "outputId": "ced49de9-3dad-4dce-c451-974e4261dcc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running run\n",
            "easily easily\n",
            "fairly fairly\n",
            "fairness fairness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#named entity recognition\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is going to build a new factory in China.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SASpZWjVQ4Z_",
        "outputId": "d8885ff6-1d1e-43b4-8f66-97c8c7b686a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "China GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring TextBlob library"
      ],
      "metadata": {
        "id": "PSqHJV3Wpxyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fmCOWYWqJSV",
        "outputId": "c4c17e75-d31e-4a43-b30b-8b15f0ca3ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"TextBlob is a simple library for processing textual data.\"\n",
        "\n",
        "# Sentiment analysis\n",
        "blob = TextBlob(text)\n",
        "print(\"TextBlob Sentiment Analysis:\")\n",
        "print(blob.sentiment)\n",
        "\n",
        "# Noun phrase extraction\n",
        "print(\"Noun phrases:\")\n",
        "print(blob.noun_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv-LO1ENm3rU",
        "outputId": "fa9c67f6-2370-4e9c-87b7-c6826f3a45b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob Sentiment Analysis:\n",
            "Sentiment(polarity=0.0, subjectivity=0.35714285714285715)\n",
            "Noun phrases:\n",
            "['textblob', 'simple library', 'processing textual data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R Libraries**"
      ],
      "metadata": {
        "id": "ZyxQiGbeShnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming using SnowballC:\n",
        "\n",
        "install.packages(\"SnowballC\")\n",
        "library(SnowballC)\n",
        "\n",
        "words <- c(\"running\", \"easily\", \"fairly\", \"fairness\")\n",
        "\n",
        "stemmed_words <- wordStem(words)\n",
        "\n",
        "print(stemmed_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NpLzBPWHqotU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2439a447-bb24-473d-cf0f-09eb3c7d814e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"run\"    \"easili\" \"fairli\" \"fair\"  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization (Sentence & Word)\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- strsplit(text, \"\\\\.\")[[1]]\n",
        "words <- unlist(strsplit(text, \"\\\\s+\"))\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "xHhlbogiqo71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc17926-4f4f-4e64-fc74-49ba19989e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[1] \"This is a sample sentence\"          \" Tokenization is important for NLP\"\n",
            "[1] \"Words:\"\n",
            " [1] \"This\"         \"is\"           \"a\"            \"sample\"       \"sentence.\"   \n",
            " [6] \"Tokenization\" \"is\"           \"important\"    \"for\"          \"NLP.\"        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "library(tokenizers)\n",
        "\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- tokenize_sentences(text)\n",
        "words <- tokenize_words(text)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "vSzXEjBtWflv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0816302-29e4-436b-da2a-997fe727e691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[[1]]\n",
            "[1] \"This is a sample sentence.\"         \"Tokenization is important for NLP.\"\n",
            "\n",
            "[1] \"Words:\"\n",
            "[[1]]\n",
            " [1] \"this\"         \"is\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"tokenization\" \"is\"           \"important\"    \"for\"          \"nlp\"         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "word_freq <- table(words)\n",
        "print(\"Word frequency:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsKw8Uddooku",
        "outputId": "73ac5191-86b7-4ed5-f8ae-f053c696d2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word frequency:\"\n",
            "words\n",
            "           a          for    important           is          nlp       sample \n",
            "           1            1            1            2            1            1 \n",
            "    sentence         this tokenization \n",
            "           1            1            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords & punctuations\n",
        "stop_words <- c(\"is\", \"a\", \"for\")  # Example list of stopwords\n",
        "filtered_words <- words[!tolower(words) %in% stop_words & !grepl(\"[[:punct:]]\", words)]\n",
        "print(\"Filtered words:\")\n",
        " print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoF00Zhmoont",
        "outputId": "55d34e2d-f6d4-4500-c65e-e45037dd5466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered words:\"\n",
            "list()\n"
          ]
        }
      ]
    }
  ]
}